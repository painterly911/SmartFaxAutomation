import streamlit as st
from google.cloud import vision
import pandas as pd
import io
import os
import re
import sqlite3
from datetime import datetime
from thefuzz import process
from PIL import Image, ImageSequence

# [ì„¤ì •] êµ¬ê¸€ í‚¤ íŒŒì¼ ê²½ë¡œ
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "my-key.json"

# --- 1. ë°ì´í„°ë² ì´ìŠ¤(SQLite) ---
def init_db():
    conn = sqlite3.connect('fax_db.sqlite')
    c = conn.cursor()
    c.execute('''
        CREATE TABLE IF NOT EXISTS fax_data (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            date TEXT,
            site_name TEXT,
            item_name TEXT,
            quantity TEXT,
            remark TEXT,
            created_at TEXT
        )
    ''')
    c.execute('''
        CREATE TABLE IF NOT EXISTS master_items (
            item_name TEXT PRIMARY KEY
        )
    ''')
    conn.commit()
    conn.close()

def save_to_db(df):
    conn = sqlite3.connect('fax_db.sqlite')
    c = conn.cursor()
    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    for index, row in df.iterrows():
        c.execute('''
            INSERT INTO fax_data (date, site_name, item_name, quantity, remark, created_at)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (row['ë‚ ì§œ'], row['í˜„ì¥ëª…'], row['í’ˆëª…'], row['ìˆ˜ëŸ‰'], row['ë¹„ê³ '], now))
    conn.commit()
    conn.close()

def load_data_from_db(site_filter=None):
    conn = sqlite3.connect('fax_db.sqlite')
    query = "SELECT id, date as ë‚ ì§œ, site_name as í˜„ì¥ëª…, item_name as í’ˆëª…, quantity as ìˆ˜ëŸ‰, remark as ë¹„ê³  FROM fax_data"
    params = []
    if site_filter and site_filter != "ì „ì²´ ë³´ê¸°":
        query += " WHERE site_name = ?"
        params.append(site_filter)
    query += " ORDER BY date ASC, id ASC"
    df = pd.read_sql(query, conn, params=params)
    conn.close()
    return df

def get_all_sites():
    conn = sqlite3.connect('fax_db.sqlite')
    try:
        df = pd.read_sql("SELECT DISTINCT site_name FROM fax_data", conn)
        return df['site_name'].tolist()
    except:
        return []
    finally:
        conn.close()

def reset_db():
    conn = sqlite3.connect('fax_db.sqlite')
    c = conn.cursor()
    c.execute("DELETE FROM fax_data")
    conn.commit()
    conn.close()

# --- í’ˆëª© ë§ˆìŠ¤í„° & ì˜¤íƒ€ ë³´ì • ---
def save_master_items(df):
    conn = sqlite3.connect('fax_db.sqlite')
    c = conn.cursor()
    c.execute("DELETE FROM master_items")
    items = df.iloc[:, 0].dropna().unique().tolist()
    count = 0
    for item in items:
        if str(item).strip():
            c.execute("INSERT OR IGNORE INTO master_items (item_name) VALUES (?)", (str(item).strip(),))
            count += 1
    conn.commit()
    conn.close()
    return count

def get_master_items():
    conn = sqlite3.connect('fax_db.sqlite')
    try:
        df = pd.read_sql("SELECT item_name FROM master_items", conn)
        return df['item_name'].tolist()
    except:
        return []
    finally:
        conn.close()

def auto_correct_item_name(ocr_name, master_list):
    if not master_list: return ocr_name
    result = process.extractOne(ocr_name, master_list)
    if result:
        best_match = result[0]
        score = result[1]
        if score >= 60:
            if len(ocr_name) > len(best_match) + 3:
                return ocr_name
            return best_match
    return ocr_name

# --- 2. ì´ë¯¸ì§€ ì²˜ë¦¬ ---
def process_uploaded_file(uploaded_file):
    image = Image.open(uploaded_file)
    processed_pages = []
    for i, page in enumerate(ImageSequence.Iterator(image)):
        page = page.copy()
        if page.mode not in ('RGB', 'L'):
            page = page.convert('RGB')
        img_byte_arr = io.BytesIO()
        page.save(img_byte_arr, format='JPEG')
        img_bytes = img_byte_arr.getvalue()
        processed_pages.append((page, img_bytes))
    return processed_pages

def get_ocr_text(image_bytes):
    client = vision.ImageAnnotatorClient()
    image = vision.Image(content=image_bytes)
    response = client.document_text_detection(image=image)
    return response.full_text_annotation.text

def clean_site_name(text):
    if not text: return text
    text = re.sub(r'\s*\(', ' (', text)
    text = re.sub(r'\(\s*', '(', text)
    text = re.sub(r'\s*\)', ')', text)
    text = re.sub(r'\)(?=\S)', ') ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def parse_text_to_df(text):
    lines = text.split('\n')
    data = []
    current_date = ""
    current_site = "í˜„ì¥ëª… ë¯¸ìƒ"
   
    date_pattern = re.compile(r'20\d{2}\s*[ë…„\-\./]\s*\d{1,2}\s*[ì›”\-\./]\s*\d{1,2}')
    num_pattern = re.compile(r'\d+')
    item_pattern = re.compile(r'^(\d+)[\.\)\s]*(.+?)\s*[:;]\s*(.+)$')

    master_list = get_master_items()

    for line in lines:
        line = line.strip()
        if not line: continue

        if "FAX" in line or "ì „ì†¡" in line or "í˜ì´ì§€" in line or "ìª½" in line:
            continue
        if "2026" in line and ":" in line:
            date_match = date_pattern.search(line)
            if date_match:
                nums = num_pattern.findall(date_match.group())
                if len(nums) >= 3:
                    current_date = f"{nums[0]}-{nums[1].zfill(2)}-{nums[2].zfill(2)}"
            continue

        if not current_date:
            date_match = date_pattern.search(line)
            if date_match:
                nums = num_pattern.findall(date_match.group())
                if len(nums) >= 3:
                    current_date = f"{nums[0]}-{nums[1].zfill(2)}-{nums[2].zfill(2)}"
       
        if "í˜„ì¥" in line:
            if "ë‹´ë‹¹ì" in line or "ì—°ë½ì²˜" in line or "í†µí™”" in line: continue
            clean_line = line.replace("â€»", "").replace("_", " ").strip()
            if "(ì…ê³ ì¼" in clean_line: clean_line = clean_line.split("(ì…ê³ ì¼")[0].strip()
            if len(clean_line) < 30:
                current_site = clean_site_name(clean_line)

    if not current_date:
        current_date = datetime.now().strftime("%Y-%m-%d")

    for line in lines:
        line = line.strip()
        if "FAX" in line or "ì „ì†¡" in line: continue

        match = item_pattern.match(line)
        if match:
            raw_item_name = match.group(2).strip()
            qty_unit = match.group(3).strip()
           
            if "/" in raw_item_name or "2026" in raw_item_name: continue
            if raw_item_name.isdigit() or len(raw_item_name) < 1: continue

            corrected_name = auto_correct_item_name(raw_item_name, master_list)

            data.append({
                "ë‚ ì§œ": current_date,
                "í˜„ì¥ëª…": current_site,
                "í’ˆëª…": corrected_name,
                "ìˆ˜ëŸ‰": qty_unit,
                "ë¹„ê³ ": ""
            })

    return pd.DataFrame(data)

def sanitize_filename(name):
    if not name: return "ë‹¤ìš´ë¡œë“œ"
    return re.sub(r'[\\/*?:"<>|]', "", str(name)).strip()

# --- 3. ë©”ì¸ í™”ë©´ ---
def main():
    st.set_page_config(page_title="ìŠ¤ë§ˆíŠ¸ íŒ©ìŠ¤ ê´€ë¦¬", layout="wide")
    init_db()
   
    with st.sidebar:
        st.title("âš™ï¸ ê´€ë¦¬ ë©”ë‰´")
        st.subheader("1. í’ˆëª© DB ì—…ë°ì´íŠ¸")
        master_file = st.file_uploader("í’ˆëª© ë§ˆìŠ¤í„° ì—‘ì…€", type=['xlsx', 'xls'])
        if master_file:
            if st.button("í’ˆëª© DB ë“±ë¡í•˜ê¸°"):
                try:
                    df = pd.read_excel(master_file)
                    count = save_master_items(df)
                    st.success(f"âœ… ì´ {count}ê°œ í’ˆëª© ë“±ë¡ ì™„ë£Œ!")
                except Exception as e:
                    st.error(f"ì˜¤ë¥˜: {e}")

        st.divider()
        st.subheader("2. ë°ì´í„° ì´ˆê¸°í™”")
        if st.button("ğŸ—‘ï¸ DB ì „ì²´ ì‚­ì œ", type="primary"):
            reset_db()
            st.warning("ì´ˆê¸°í™” ì™„ë£Œ!")
            st.rerun()

    st.title("ğŸ“  ìŠ¤ë§ˆíŠ¸ íŒ©ìŠ¤ ìì¬ ê´€ë¦¬ ì‹œìŠ¤í…œ")
   
    master_count = len(get_master_items())
    if master_count > 0:
        st.info(f"ğŸ’¡ ì˜¤íƒ€ ìë™ ë³´ì • ê°€ë™ ì¤‘ (ë“±ë¡ëœ í’ˆëª©: {master_count}ê°œ)")

    tab1, tab2 = st.tabs(["ğŸ“¤ íŒ©ìŠ¤ ë“±ë¡í•˜ê¸°", "ğŸ“Š ëˆ„ì  ë‚´ì—­ ì¡°íšŒ"])

    with tab1:
        st.write("ì´ë¯¸ì§€ ë˜ëŠ” TIFF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
        uploaded_file = st.file_uploader("íŒ©ìŠ¤ íŒŒì¼ ì„ íƒ", type=['jpg', 'png', 'jpeg', 'tiff', 'tif'])

        if uploaded_file is not None:
            pages = process_uploaded_file(uploaded_file)
            st.write(f"ğŸ“„ ì´ **{len(pages)}í˜ì´ì§€** ê°ì§€")

            col1, col2 = st.columns([1, 2])
            with col1:
                for i, (disp_img, _) in enumerate(pages):
                    st.image(disp_img, caption=f'{i+1} í˜ì´ì§€', use_column_width=True)
           
            with col2:
                if st.button("ì „ì²´ í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ", type="primary"):
                    with st.spinner('ë¶„ì„ ì¤‘...'):
                        try:
                            all_dfs = []
                            for i, (_, img_bytes) in enumerate(pages):
                                raw_text = get_ocr_text(img_bytes)
                                df = parse_text_to_df(raw_text)
                                if not df.empty: all_dfs.append(df)
                           
                            if all_dfs:
                                final_df = pd.concat(all_dfs, ignore_index=True)
                                st.session_state['temp_df'] = final_df
                                st.success(f"ë¶„ì„ ì™„ë£Œ! ({len(final_df)}ê±´)")
                            else:
                                st.warning("ë‚´ìš© ì—†ìŒ")
                        except Exception as e:
                            st.error(f"ì˜¤ë¥˜: {e}")

            if 'temp_df' in st.session_state:
                st.divider()
                st.subheader("ğŸ§ ë°ì´í„° í™•ì¸")
                edited_df = st.data_editor(
                    st.session_state['temp_df'],
                    num_rows="dynamic",
                    use_container_width=True,
                    column_config={
                        "í’ˆëª…": st.column_config.TextColumn("í’ˆëª…", width="large"),
                        "ìˆ˜ëŸ‰": st.column_config.TextColumn("ìˆ˜ëŸ‰", width="medium"),
                    }
                )

                # [ìˆ˜ì •] ë‹¤ìš´ë¡œë“œ ì˜ì—­ ë””ìì¸ ê°œì„ 
                st.markdown("---")
                save_col1, save_col2 = st.columns(2)
               
                with save_col1:
                    st.caption("1. ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥")
                    if st.button("ğŸ’¾ DBì— ëˆ„ì í•˜ê¸°", use_container_width=True):
                        save_to_db(edited_df)
                        st.success("âœ… ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
                        del st.session_state['temp_df']
                        st.rerun()
               
                with save_col2:
                    st.caption("2. ì—‘ì…€ë¡œ ì¦‰ì‹œ ë‹¤ìš´ë¡œë“œ")
                    # ê¸°ë³¸ íŒŒì¼ëª… ì„¤ì •
                    default_name = "ë‹¤ìš´ë¡œë“œ"
                    if not edited_df.empty:
                        default_name = sanitize_filename(edited_df['í˜„ì¥ëª…'].iloc[0])

                    # [í•µì‹¬] ì‚¬ìš©ìê°€ ì§ì ‘ ì…ë ¥í•  ìˆ˜ ìˆëŠ” ì¹¸
                    user_filename = st.text_input("íŒŒì¼ ì´ë¦„ (ì§ì ‘ ìˆ˜ì • ê°€ëŠ¥)", value=default_name)
                   
                    # í™•ì¥ì .xlsx ìë™ ë³´ì •
                    if not user_filename.endswith(".xlsx"):
                        user_filename += ".xlsx"

                    output = io.BytesIO()
                    with pd.ExcelWriter(output, engine='openpyxl') as writer:
                        edited_df.to_excel(writer, index=False, sheet_name='íŒ©ìŠ¤ë‚´ì—­')
                       
                    st.download_button(
                        label="ğŸ“¥ ì—‘ì…€ íŒŒì¼ ë°›ê¸°",
                        data=output.getvalue(),
                        file_name=user_filename, # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì´ë¦„ ì ìš©
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                        use_container_width=True
                    )

    with tab2:
        st.subheader("ğŸ” í˜„ì¥ë³„ ìì¬ ë‚´ì—­ ì¡°íšŒ")
        site_list = ["ì „ì²´ ë³´ê¸°"] + get_all_sites()
        selected_site = st.selectbox("í˜„ì¥ ì„ íƒ", site_list)
        history_df = load_data_from_db(selected_site)
       
        if not history_df.empty:
            st.write(f"ì´ **{len(history_df)}**ê±´")
            st.dataframe(history_df, use_container_width=True, hide_index=True)
           
            # ê¸°ë³¸ íŒŒì¼ëª…
            if selected_site == "ì „ì²´ ë³´ê¸°":
                default_dl_name = "ì „ì²´_ìì¬ë‚´ì—­"
            else:
                default_dl_name = sanitize_filename(selected_site)
           
            # [í•µì‹¬] ì¡°íšŒ íƒ­ì—ì„œë„ ì´ë¦„ ìˆ˜ì • ê°€ëŠ¥
            col_dn1, col_dn2 = st.columns([3, 1])
            with col_dn1:
                user_dl_name = st.text_input("ë‹¤ìš´ë¡œë“œ íŒŒì¼ ì´ë¦„", value=default_dl_name)
                if not user_dl_name.endswith(".xlsx"):
                    user_dl_name += ".xlsx"
           
            with col_dn2:
                # ì¤„ë§ì¶¤ì„ ìœ„í•œ ë¹ˆ ê³µê°„
                st.write("")
                st.write("")
               
                output = io.BytesIO()
                with pd.ExcelWriter(output, engine='openpyxl') as writer:
                    history_df.to_excel(writer, index=False, sheet_name='ìì¬ë‚´ì—­')
               
                st.download_button(
                    label="ğŸ“¥ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
                    data=output.getvalue(),
                    file_name=user_dl_name,
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    use_container_width=True
                )
        else:
            st.info("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()